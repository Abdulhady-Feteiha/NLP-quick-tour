{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP interview.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZxG5ctUDfESnFaoQeL09B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdulhady-Feteiha/NLP-interview-code-questions/blob/main/NLP_interview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "gSQyohO2qHqv"
      },
      "outputs": [],
      "source": [
        "#imports \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You know, some donwloads"
      ],
      "metadata": {
        "id": "Tv0AD0Uh4gw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "!python -m spacy download en_core_web_md "
      ],
      "metadata": {
        "id": "O2TqV0tgviA7",
        "outputId": "00ecdec7-2716-41ce-b4be-ee5ae3aeaf52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-07-30 01:54:42.235772: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.4.0/en_core_web_md-3.4.0-py3-none-any.whl (42.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.8 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.9.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-md==3.4.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokeniaztion | nltk\n"
      ],
      "metadata": {
        "id": "sp-_kzHbqmfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The_words_of_god = \"Hey there, smile. the univers is smiling back.\"\n",
        "sent_Tokenized = sent_tokenize(The_words_of_god)\n",
        "word_Tokenized = word_tokenize(The_words_of_god)\n",
        "\n",
        "print('sent_tokenize: ',sent_Tokenized)\n",
        "print('word_Tokenized: ',word_Tokenized)"
      ],
      "metadata": {
        "id": "x5B4qKhgqhfl",
        "outputId": "ec6e89e2-6fb1-407c-fac9-d0f32cef732b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sent_tokenize:  ['Hey there, smile.', 'the univers is smiling back.']\n",
            "word_Tokenized:  ['Hey', 'there', ',', 'smile', '.', 'the', 'univers', 'is', 'smiling', 'back', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-grams | nltk"
      ],
      "metadata": {
        "id": "uZq-5ocWqvMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  n = 4\n",
        "  print(list(nltk.ngrams(word_Tokenized,n)))"
      ],
      "metadata": {
        "id": "5lE_0nf8qtBS",
        "outputId": "1088a0a2-6fdd-4155-b9ac-4ebbe5ed4acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hey', 'there', ',', 'smile'), ('there', ',', 'smile', '.'), (',', 'smile', '.', 'the'), ('smile', '.', 'the', 'univers'), ('.', 'the', 'univers', 'is'), ('the', 'univers', 'is', 'smiling'), ('univers', 'is', 'smiling', 'back'), ('is', 'smiling', 'back', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming | nltk"
      ],
      "metadata": {
        "id": "FjboIDBRq1FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pst=PorterStemmer()\n",
        "for token in word_Tokenized:\n",
        "  print(pst.stem(token))"
      ],
      "metadata": {
        "id": "CduuiNrkq2xT",
        "outputId": "245a51ec-bdd1-4c86-c9a4-ed7258ab14fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey\n",
            "there\n",
            ",\n",
            "smile\n",
            ".\n",
            "the\n",
            "univ\n",
            "is\n",
            "smile\n",
            "back\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization | nltk"
      ],
      "metadata": {
        "id": "OhOIuO9Etqe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemma= WordNetLemmatizer()\n",
        "for token in word_Tokenized:\n",
        "  print(token + \":\" + lemma.lemmatize(token))"
      ],
      "metadata": {
        "id": "sOSdyqdSsCBe",
        "outputId": "1deb2deb-0bcb-484f-9ebf-05ff87e51601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey:Hey\n",
            "there:there\n",
            ",:,\n",
            "smile:smile\n",
            ".:.\n",
            "the:the\n",
            "univers:univers\n",
            "is:is\n",
            "smiling:smiling\n",
            "back:back\n",
            ".:.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS | nltk"
      ],
      "metadata": {
        "id": "RUmnw4c6u6eW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "txt = \"Sourav, Pratyush, and Abhinav are good friends.\"  \n",
        "\n",
        "wordsList = [w for w in word_Tokenized if not w in stop_words]\n",
        "print(\"Stop words removed:\")\n",
        "print(wordsList)\n",
        "print(\"POS:\")\n",
        "tagged_words = nltk.pos_tag(wordsList)\n",
        "print(tagged_words)"
      ],
      "metadata": {
        "id": "6PP-iEoUuF2u",
        "outputId": "1573e1ac-5b69-45d7-e1f3-4a9a858c56b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop words removed:\n",
            "['Hey', ',', 'smile', '.', 'univers', 'smiling', 'back', '.']\n",
            "POS:\n",
            "[('Hey', 'NNP'), (',', ','), ('smile', 'NN'), ('.', '.'), ('univers', 'NNS'), ('smiling', 'VBG'), ('back', 'RB'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER | SpaCy"
      ],
      "metadata": {
        "id": "9Y9VR88xxCN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "NER = spacy.load(\"en_core_web_sm\")\n",
        "raw_text=\"I was contracted by Udacity inc. for a year as a tutor in their EG-FWD program.\"\n",
        "text1= NER(raw_text)\n",
        "print(\"NER output:\")\n",
        "for word in text1.ents:\n",
        "    print(word.text,word.label_)\n",
        "print(\"What is an ORG? \\n\",spacy.explain(\"ORG\"))\n"
      ],
      "metadata": {
        "id": "9h6ZxKorvXEf",
        "outputId": "89db7c84-b93b-407b-8d8e-6f7e6daa5865",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NER output:\n",
            "Udacity inc. ORG\n",
            "a year DATE\n",
            "EG GPE\n",
            "What is an ORG? \n",
            " Companies, agencies, institutions, etc.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(text1,style=\"ent\",jupyter=True)"
      ],
      "metadata": {
        "id": "lNoVQriLwRxk",
        "outputId": "70bbc464-5b3a-43c3-9aba-dbfe69b0051d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">I was contracted by \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Udacity inc.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " for \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    a year\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " as a tutor in their \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    EG\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              "-FWD program.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity | SpaCy\n"
      ],
      "metadata": {
        "id": "M4t7adnmy_cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_md')\n",
        "\n",
        "Sentence1 = nlp('The ancient Egyptian civilization were the first to disover the color blue')\n",
        "Sentence2 = nlp('The pharoas discovered the color blue first')\n",
        "Sentence3 = nlp('Blue describes a shade of wavelengths ragning around 6.66x10^14 Hz')\n",
        "Sentence4 = nlp('Dogs are cute man!')\n",
        "print(\"Two similar sentences:\", Sentence1.similarity(Sentence2))\n",
        "print(\"Two simi similar sentences:\", Sentence1.similarity(Sentence3))\n",
        "print(\"Two non similar sentences:\", Sentence1.similarity(Sentence4))"
      ],
      "metadata": {
        "id": "zo1GcP7ny1iy",
        "outputId": "bc2ea8f3-e8d0-416d-c6b6-eafe576e6fcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two similar sentences: 0.8835315773093688\n",
            "Two simi similar sentences: 0.6293763995810023\n",
            "Two non similar sentences: 0.19793447908323122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eqI4bJvNzivJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}